{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "- OS `os`\n",
    "- MLFlow `mlflow`\n",
    "- PyTorch, PyTorch Vision `torch`\n",
    "- Optuna `optuna`\n",
    "\n",
    "---\n",
    "\n",
    "# Project Definition\n",
    "\n",
    "In the **Digikala** sellers panel, when sellers add their products to the website for\n",
    "sale, they must also submit a number of images of that product for each product; But not\n",
    "every image can be appropriate because images must meet a predetermined conditions.\n",
    "One of these conditions is the absence of\n",
    "any watermarks on the image. In this issue,\n",
    "we ask you to use the data provided to you to train a model that is able to detect\n",
    "the presence of watermarks.\n",
    "\n",
    "One of these conditions is the **absence of any watermarks** on the\n",
    "image. In this case, we want to use the data we have to teach a model\n",
    "that is able to detect the presence of watermarks.\n",
    "\n",
    "The data set we have has **training** and **test** parts. In the training part,\n",
    "a set of images that have a watermark are in a\n",
    "`positive` folder and a set of images that\n",
    "do not have any watermark are in a `negative` folder.\n",
    "\n",
    "Using these images, we train our machine learning algorithm\n",
    "and then predict whether each of the images in the test folder has a watermark.\n",
    "\n",
    "---\n",
    "\n",
    "# Learning Model\n",
    "\n",
    "In this project, we use **Inception V3** **End-To-End** pre-trained deep neural network with the help of **Transfer Learning** technique.\n",
    "\n",
    "Transfer learning brings a range of benefits to the development process of machine learning models. The main benefits of transfer learning include the saving of resources and improved efficiency when training new models.\n",
    "\n",
    "> [End-to-End Models](https://www.capitalone.com/tech/machine-learning/pros-and-cons-of-end-to-end-models/)\n",
    "\n",
    "> [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks.)\n",
    "\n",
    "End-to-end models have a number of advantages relative to component-based systems, but they also have some disadvantages.\n",
    "\n",
    "Advantages of end-to-end models:\n",
    "- **Better metrics**: Currently, the systems with the best performance according to metrics such as precision and recall tend to be end-to-end models.\n",
    "- **Simplicity**: End-to-end models avoid the sometimes thorny problem of determining which components are needed to perform a task and how those components interact. In component-based systems, if the output format of one component is changed, the input format of other components may need to be revised.\n",
    "- **Reduced effort**: End-to-end models arguably require less work to create than component-based systems. Component-based systems require a larger number of design choices.\n",
    "- **Applicability to new tasks**: End-to-end models can potentially work for a new task simply by retraining using new data. Component-based systems may require significant re-engineering for new tasks.\n",
    "- **Ability to leverage naturally-occurring data**: End-to-end models can be trained on existing data, such as translations of works from one language to another, or logs of customer service agent chats and actions. Component-based systems may require creation of new labeled data to train each component.\n",
    "- **Optimization**: End-to-end models are optimized for the entire task. Optimization of a component-based system is difficult. Errors accumulate across components, with a mistake in one component affecting downstream components. Information from downstream components canâ€™t inform upstream components.\n",
    "- **Lower degree of dependency on subject matter experts**: End-to-end models can be trained on naturally-occurring data, which reduces the need for specialized linguistic and domain knowledge. But expertise in deep neural networks is often required.\n",
    "- **Ability to fully leverage machine learning**: End-to-end models take the idea of machine learning to the limit.\n",
    "\n",
    "---\n",
    "\n",
    "# Code\n",
    "\n",
    "The main classes that implement the overall logic of the code are as follows:\n",
    "\n",
    "1. **`HyperParameterOptimization` class**: This class is\n",
    "    responsible for Hyper Parameter Fine-Tuning with the help\n",
    "    of `ModelInitializer` and `ModelTrainer` classes and uses\n",
    "    `optuna` library for this responsibility.\n",
    "\n",
    "    The main function of this class is `tune`, which defines\n",
    "    an `study` on the hyper parameter value space. To define\n",
    "    `study` in the `optuna` library, it is necessary to define\n",
    "    the `objective_function` manually.\n",
    "\n",
    "  ```\n",
    "  def tune(self):\n",
    "      study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "\n",
    "      study.optimize(self.hyper_parameter_optimization_objective_function, n_trials=self.n_trial)\n",
    "\n",
    "      best_trial = study.best_trial\n",
    "\n",
    "      params = best_trial.params\n",
    "\n",
    "      print('*' * 160)\n",
    "      for k, v in params.items():\n",
    "          print(\"{:<15}{:<25}\".format(k, str(v)))\n",
    "\n",
    "      print('*' * 160)\n",
    "\n",
    "      hyperparameters_configs = HyperParameterConfigs(params)\n",
    "      hyperparameters_configs.set_metrics()\n",
    "\n",
    "      self.model_initializer.initialize_with_hyper_parameters(hyperparameters_configs)\n",
    "\n",
    "      _ = self.model_trainer.fit_model_with_setup(self.model_initializer.train_dataloader,\n",
    "                                                  self.model_initializer.validation_dataloader,\n",
    "                                                  self.model_initializer.model,\n",
    "                                                  self.model_initializer.loss_criterion,\n",
    "                                                  self.model_initializer.optimizer,\n",
    "                                                  self.model_initializer.device)\n",
    "\n",
    "      fine_tuned_model = self.model_initializer.model\n",
    "\n",
    "      return fine_tuned_model, hyperparameters_configs\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "2. **`ModelInitializer` class**: This class implements the following step:\n",
    "    1. In the first step, it reads the data of the\n",
    "    training phase from the file and then divides it to the\n",
    "     training and validation subset (pytorch `Subset` class)\n",
    "     and in the next step, it converts this `Subset` instances\n",
    "     to pytorch `Dataset` class by applying some transforms.\n",
    "\n",
    "    2. In the second step, it applies\n",
    "     transforms to the training\n",
    "    and validation subset as follows:\n",
    "\n",
    "        ```\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize(size=image_data_shape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=probability),\n",
    "            transforms.RandomPerspective(p=probability, distortion_scale=.5),\n",
    "            transforms.Normalize(mean, standard_deviation)\n",
    "        ])\n",
    "\n",
    "        validation_transforms = transforms.Compose([\n",
    "            transforms.Resize(size=image_data_shape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, standard_deviation)\n",
    "        ])\n",
    "\n",
    "        train_dataset = DatasetFromSubset(self.train_subset, transform=train_transforms)\n",
    "        validation_dataset = DatasetFromSubset(self.validation_subset, transform=validation_transforms)\n",
    "        ```\n",
    "    3. In the third step, with the help of the datasets created\n",
    "    in the previous step, it builds the training and validation dataloader.\n",
    "    (pytorch `DataLoader` class, in the model training process, breaks down\n",
    "    training and validation datasets into mini-batches of `batch_size` size.)\n",
    "\n",
    "        ```\n",
    "        # come from hyper parameter configs object as input\n",
    "        batch_size = hp_configs.batch_size\n",
    "        shuffle = True\n",
    "        drop_last = False\n",
    "        num_workers = 4\n",
    "\n",
    "        self.train_dataloader = data.DataLoader(train_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                drop_last=drop_last,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "        self.validation_dataloader = data.DataLoader(validation_dataset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=shuffle,\n",
    "                                                     drop_last=drop_last,\n",
    "                                                     num_workers=num_workers,\n",
    "                                                     pin_memory=True)\n",
    "        ```\n",
    "    4. In the fourth step, the following\n",
    "       items are calculated using the hyper\n",
    "       parameter values given as input (`HyperParameterConfigs` class):\n",
    "\n",
    "        * Device `torch.device`:\n",
    "        ```\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        ```\n",
    "        * Model `torchvision.models.Inception3`\n",
    "        ```\n",
    "        model: models.Inception3 = models.inception_v3(pretrained=True, progress=True)\n",
    "\n",
    "        last_fc_layer_input_number = model.fc.in_features\n",
    "        model.fc = nn.Linear(last_fc_layer_input_number, number_of_classes)\n",
    "\n",
    "        model = model.to(self.device)\n",
    "\n",
    "        self.model = model\n",
    "        ```\n",
    "        * Optimizer `torch.optim.Adam`\n",
    "        ```\n",
    "        self.optimizer = optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        ```\n",
    "        * Loss Criterion `torch.nn.CrossEntropyLoss`\n",
    "        ```\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        ```\n",
    "        * Learning Rate Scheduler `torch.optim.lr_scheduler.ExponentialLR`\n",
    "        ```\n",
    "        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=scheduler_gamma)\n",
    "        ```\n",
    "\n",
    "   In the following, all these items are passed to `ModelTrainer` class object as input.\n",
    "\n",
    "3. **`ModelTrainer` class**: This class, using the items created\n",
    "    in the previous section, conducts the neural network model\n",
    "    training process.\n",
    "\n",
    "    The main function of this class is `fit_model_with_setup`,\n",
    "    which the main part of this function is as follows:\n",
    "\n",
    "   ```\n",
    "   for epoch_i in range(self.epochs):\n",
    "       train_true_labels, train_predicted_scores = self.train_loop()\n",
    "       test_true_labels, test_predicts_proba = self.test_loop()\n",
    "\n",
    "       train_auc, train_accuracy, train_f_score, train_loss = self.calculate_metrics(train_true_labels, train_predicted_scores)\n",
    "       validation_auc, validation_accuracy, validation_f_score, validation_loss = self.calculate_metrics(test_true_labels, test_predicts_proba)\n",
    "\n",
    "       current_epoch_metrics_dictionary = {\n",
    "           TRAIN_AUC_STR: train_auc,\n",
    "           TRAIN_ACCURACY_STR: train_accuracy,\n",
    "           TRAIN_F1SCORE_STR: train_f_score,\n",
    "           TRAIN_LOSS_STR: train_loss,\n",
    "\n",
    "           VALIDATION_AUC_STR: validation_auc,\n",
    "           VALIDATION_ACCURACY_STR: validation_accuracy,\n",
    "           VALIDATION_F1SCORE_STR: validation_f_score,\n",
    "           VALIDATION_LOSS_STR: validation_loss,\n",
    "        }\n",
    "\n",
    "       metrics_tracker.insert(epoch_i, current_epoch_metrics_dictionary)\n",
    "\n",
    "       mlflow.log_metrics(current_epoch_metrics_dictionary)\n",
    "\n",
    "       print(\"{:<15}{:<25}{:<25}{:<25}{:<25}{:<25}{:<25}\".format(epoch_i + 1, train_loss, validation_loss, train_f_score, validation_f_score, train_accuracy, validation_accuracy))\n",
    "       print('-' * 160)\n",
    "    ```\n",
    "4. **`ModelEvaluator` class**: This class, using the model\n",
    " obtained in the previous section, calculates\n",
    " the label of test data that has not been used\n",
    "  in the training process at all.\n",
    "\n",
    "---\n",
    "\n",
    "# Results\n",
    "\n",
    "With the help of **MLFlow** and by executing the following command, the results of the training process can be seen in the browser."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "mlflow server -p 5050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Predict Test Data Labels \n",
    "\n",
    "The prediction of the trained model on test data (that has not been used in the training process.) is\n",
    "stored in the `output.csv` file."
   ]
  }
 ],
 "metadata": {
  "direction": "ltr",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
